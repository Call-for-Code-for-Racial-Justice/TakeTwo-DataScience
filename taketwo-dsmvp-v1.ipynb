{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a variant of \"taketwo-dsmvp-v1\" for the Jigsaw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is also a variant of the version that was pushed to the main branch of TakeTwo repo in Feb 21. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a version of \"v1\" demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE DEPENDENCY: sklearn, numpy, pandas, random, string. It uses from sklearn NB (Naive Bayes) and SGD (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA DEPENDENCY: In this version the \"Jigsaw unintended biased in toxicity\" data is used for demo purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data?select=train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for detailed description of the data and instructions to download them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As of now we are using only the \"train.csv\" portion of the data, which needs to be in the calling directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the Jigsaw data and store them in appropriate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsawTrain = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsawTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrain = pd.DataFrame(jigsawTrain, columns = ['id', 'comment_text', 'severe_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 3)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrain1M = allTrain.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrain1M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allTrain = allTrain1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackData = jigsawTrain[(jigsawTrain.black == 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackTrain = pd.DataFrame(blackData, columns = ['id', 'comment_text', 'severe_toxicity', 'black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7656, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackTrainId = blackTrain['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackTrainText = blackTrain['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackTrainSevereToxicity = blackTrain.loc[:,'severe_toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516    BJ, your suggestion that low income residents ...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackTrainText.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrainId = allTrain['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrainText = allTrain['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrainSevereToxicity = allTrain.loc[:,'severe_toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000000\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.000000\n",
       "4    0.021277\n",
       "Name: severe_toxicity, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrainSevereToxicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>black</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>240627</td>\n",
       "      <td>BJ, your suggestion that low income residents ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>242346</td>\n",
       "      <td>Well, this wasn't meant to be a dissertation. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>242454</td>\n",
       "      <td>So Hillary's appeal is to Democrats who make o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>242498</td>\n",
       "      <td>Is the real point of this article to show that...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>242562</td>\n",
       "      <td>&gt; A less sensational headline would be “Black ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                       comment_text  severe_toxicity  black\n",
       "516   240627  BJ, your suggestion that low income residents ...              0.0    1.0\n",
       "1428  242346  Well, this wasn't meant to be a dissertation. ...              0.0    1.0\n",
       "1527  242454  So Hillary's appeal is to Democrats who make o...              0.0    1.0\n",
       "1569  242498  Is the real point of this article to show that...              0.0    1.0\n",
       "1623  242562  > A less sensational headline would be “Black ...              0.0    1.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackTrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text  severe_toxicity\n",
       "0  59848  This is so cool. It's like, 'would you want yo...         0.000000\n",
       "1  59849  Thank you!! This would make my life a lot less...         0.000000\n",
       "2  59852  This is such an urgent design problem; kudos t...         0.000000\n",
       "3  59855  Is this something I'll be able to install on m...         0.000000\n",
       "4  59856               haha you guys are a bunch of losers.         0.021277"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240627"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackTrainId.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BJ, your suggestion that low income residents of Portland do not understand their own needs and desires perplexes me.  It is a bit snobby- as if the poor need middle class whites to tell them what they should want. And, for blacks and latinos, home ownership[ is pretty much their only chance for a nest egg.  Unless they are union they don't really get the jobs with stock options and generous 401k matches.  The fact that redlining once existed doesn't make the handful of black residents around Alberta less attached to their homes.\\n\\nYou are struggling with he fact that the working class folks in Portland do not want to live the New Urbanist dream.  Look at the big study Metro did to gauge housing preferences. \\n\\nFinally, I note you put a lot of trust in the people who own large apartment complexes to not exploit the inherent vulnerability that comes with being a renter.  The SFH gives people autonomy.  Yes, housing prices can drop, but so can any other investment.  Take my 401k- please...\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackTrainText.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackTrainSevereToxicity.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7656"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blackTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"non-toxic\", \"toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "data_set = namedtuple('data_set', [\"data\", \"urls\", \"target_names\", \"target\"])([],[],categories,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipCoin(bias):\n",
    "    f = random.random()\n",
    "    return True if f<bias else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(flipCoin(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(blackTrain)):\n",
    "    if (blackTrainSevereToxicity.iloc[x] > 0):\n",
    "        data_set.target.append(1)\n",
    "        data_set.urls.append(blackTrainId.iloc[x])\n",
    "        data_set.data.append(blackTrainText.iloc[x])\n",
    "    elif flipCoin(0.25):\n",
    "        data_set.target.append(0)\n",
    "        data_set.urls.append(blackTrainId.iloc[x])\n",
    "        data_set.data.append(blackTrainText.iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "all_data_set = namedtuple('data_set', [\"data\", \"urls\", \"target_names\", \"target\"])([],[],categories,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "all_train_set = namedtuple('data_set', [\"data\", \"urls\", \"target_names\", \"target\"])([],[],categories,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "all_test_set = namedtuple('data_set', [\"data\", \"urls\", \"target_names\", \"target\"])([],[],categories,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "downSampling = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(allTrain)):\n",
    "    if (allTrainSevereToxicity.iloc[x] > 0):\n",
    "        all_data_set.target.append(1)\n",
    "        all_data_set.urls.append(allTrainId.iloc[x])\n",
    "        all_data_set.data.append(allTrainText.iloc[x])\n",
    "    elif flipCoin(downSampling):\n",
    "        all_data_set.target.append(0)\n",
    "        all_data_set.urls.append(allTrainId.iloc[x])\n",
    "        all_data_set.data.append(allTrainText.iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a version in which training and test data are separately generated from all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRatio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(allTrain)):\n",
    "    if (allTrainSevereToxicity.iloc[x] > 0):\n",
    "        if flipCoin(trainRatio): \n",
    "            all_train_set.target.append(1)\n",
    "            all_train_set.urls.append(allTrainId.iloc[x])\n",
    "            all_train_set.data.append(allTrainText.iloc[x])\n",
    "        else: \n",
    "            all_test_set.target.append(1)\n",
    "            all_test_set.urls.append(allTrainId.iloc[x])\n",
    "            all_test_set.data.append(allTrainText.iloc[x])\n",
    "    elif flipCoin(downSampling):\n",
    "        if flipCoin(trainRatio): \n",
    "            all_train_set.target.append(0)\n",
    "            all_train_set.urls.append(allTrainId.iloc[x])\n",
    "            all_train_set.data.append(allTrainText.iloc[x])\n",
    "        else: \n",
    "            all_test_set.target.append(0)\n",
    "            all_test_set.urls.append(allTrainId.iloc[x])\n",
    "            all_test_set.data.append(allTrainText.iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#either leave data_set as is (generated from blackData) or set it to all_data_set (generated from all the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = all_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20830"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data_set.target[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10714"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59861, 239607, 239986, 240424, 240546, 240598, 240781, 240782, 240879, 241315]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.urls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hahahahahahahahhha suck it.',\n",
       " \"Yet call out all Muslims for the acts of a few will get you pilloried.   So why is it okay to smear an entire religion over these few idiots?  Or is this because it's okay to bash Christian sects?\"]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_train = all_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here a separate test data set should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_test = all_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "non-toxic\n",
      "non-toxic\n",
      "non-toxic\n",
      "toxic\n",
      "non-toxic\n",
      "non-toxic\n"
     ]
    }
   ],
   "source": [
    "for t in marker_train.target[:10]:\n",
    "...     print(marker_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marker_train.data is an array of strings (containing as many \"texts\" as there are coming from Marker.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marker_train.target is an array of integers (containing the class indices for the above texts' class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marker_train_target_names is an array of class label names indexed by the array indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186506"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(marker_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93922"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(marker_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20830"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(marker_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "non-toxic\n",
      "non-toxic\n",
      "non-toxic\n",
      "toxic\n",
      "non-toxic\n",
      "non-toxic\n"
     ]
    }
   ],
   "source": [
    "for t in marker_train.target[:10]:\n",
    "...     print(marker_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "toxic\n",
      "non-toxic\n",
      "non-toxic\n",
      "toxic\n",
      "non-toxic\n"
     ]
    }
   ],
   "source": [
    "for t in marker_test.target[:10]:\n",
    "...     print(marker_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes for model representations packaging the components in the modeling pipeline.\n",
    "#For SGDClassifier and NaiveBayes.\n",
    "#We may want to extend it to utilize \"Pipeline()\" as in \"ModelRepSGD0\", which is not in use at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRepSGD:\n",
    "    def __init__(self, count_vect, X_train_counts, tf_transformer, text_clf):\n",
    "        self.count_vect = CountVectorizer()\n",
    "        self.X_train_counts = self.count_vect.fit_transform(marker_train.data)\n",
    "        self.tf_transformer = TfidfTransformer(use_idf=False).fit(self.X_train_counts)\n",
    "        self.text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                  alpha=1e-3, random_state=42,\n",
    "                                  max_iter=5, tol=None)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRepNB:\n",
    "    def __init__(self, count_vect, X_train_counts, tf_transformer, clf):\n",
    "        self.count_vect = CountVectorizer()\n",
    "        self.X_train_counts = self.count_vect.fit_transform(marker_train.data)\n",
    "        self.tf_transformer = TfidfTransformer(use_idf=False).fit(self.X_train_counts)\n",
    "        self.clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for model training - for SGDClassifier and NaiveBayes (SGD one is untested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_SGD(marker_train): \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(marker_train.data)\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                              alpha=1e-3, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "    ])\n",
    "    text_clf.fit(marker_train.data, marker_train.target)\n",
    "    trained_model = ModelRepSGD(count_vect, X_train_counts, tf_transformer, text_clf)\n",
    "    trained_model.text_clf = text_clf\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_NB(marker_train): \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(marker_train.data)\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "    clf = MultinomialNB().fit(X_train_tf, marker_train.target)\n",
    "    trained_model = ModelRepNB(count_vect, X_train_counts, tf_transformer, clf)\n",
    "    trained_model.clf = clf\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for model testing (common for SGDClassifier and NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(flagger_new_test, trained_model):\n",
    "    count_vect = trained_model.count_vect\n",
    "    tf_transformer = trained_model.tf_transformer\n",
    "    clf = trained_model.clf\n",
    "    X_new_counts = count_vect.transform(flagger_new_test)\n",
    "    X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "    predicted = clf.predict(X_new_tfidf)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now testing model_train and model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_NB = model_train_NB(marker_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_SGD = model_train_SGD(marker_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the components of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ModelRepNB"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_model_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_NB.count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(use_idf=False)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_NB.tf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_NB.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_SGD.count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(use_idf=False)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_SGD.tf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
       "                               tol=None))])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_SGD.text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_SGD.count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(use_idf=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_SGD.tf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling model tester/predictor on a single new text from Flagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flagger_new_text = fetch_20newsgroups(subset='test',\n",
    "#                                      categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagger_new_text = ['haha you guys are a bunch of losers.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_NB = model_test(flagger_new_text, trained_model_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_SGD = trained_model_SGD.text_clf.predict(flagger_new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now evaluating NB and SGD models on a test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_NB = model_test(marker_test.data, trained_model_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_SGD = trained_model_SGD.text_clf.predict(marker_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20830,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_SGD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predicted_test_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11114"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predicted_test_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_SGD[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marker_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622659625540087"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted_test_NB == marker_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7121939510321651"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted_test_SGD == marker_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ibm_watson_machine_learning in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (1.0.100)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas<1.3.0,>=0.24.2 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk==2.7.* in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: lomond in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm_watson_machine_learning) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from requests->ibm_watson_machine_learning) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from requests->ibm_watson_machine_learning) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from pandas<1.3.0,>=0.24.2->ibm_watson_machine_learning) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from pandas<1.3.0,>=0.24.2->ibm_watson_machine_learning) (1.19.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from pandas<1.3.0,>=0.24.2->ibm_watson_machine_learning) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk-s3transfer==2.7.0 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (2.7.0)\n",
      "Processing /Users/upkar/Library/Caches/pip/wheels/54/a5/95/ac2068077b0f41719c2a69a03fdfbdd76ad02cdbb0984ad1a7/ibm_cos_sdk_core-2.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from lomond->ibm_watson_machine_learning) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from packaging->ibm_watson_machine_learning) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (0.15.2)\n",
      "Installing collected packages: ibm-cos-sdk-core\n",
      "  Attempting uninstall: ibm-cos-sdk-core\n",
      "    Found existing installation: ibm-cos-sdk-core 2.10.0\n",
      "    Uninstalling ibm-cos-sdk-core-2.10.0:\n",
      "      Successfully uninstalled ibm-cos-sdk-core-2.10.0\n",
      "Successfully installed ibm-cos-sdk-core-2.7.0\n",
      "Processing /Users/upkar/Library/Caches/pip/wheels/ce/df/1e/1da07def7155e0dd5cb2d93925ac6c0026042ecd3d3411ed9e/ibm_cos_sdk_core-2.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm-cos-sdk-core) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0,>=2.18 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm-cos-sdk-core) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from ibm-cos-sdk-core) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/upkar/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core) (1.15.0)\n",
      "\u001b[31mERROR: ibm-cos-sdk 2.7.0 has requirement ibm-cos-sdk-core==2.7.0, but you'll have ibm-cos-sdk-core 2.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: ibm-cos-sdk-s3transfer 2.7.0 has requirement ibm-cos-sdk-core==2.7.0, but you'll have ibm-cos-sdk-core 2.10.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: ibm-cos-sdk-core\n",
      "  Attempting uninstall: ibm-cos-sdk-core\n",
      "    Found existing installation: ibm-cos-sdk-core 2.7.0\n",
      "    Uninstalling ibm-cos-sdk-core-2.7.0:\n",
      "      Successfully uninstalled ibm-cos-sdk-core-2.7.0\n",
      "Successfully installed ibm-cos-sdk-core-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ibm_watson_machine_learning\n",
    "!pip install -U ibm-cos-sdk-core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add apikey here\n",
    "wml_credentials = {\n",
    "                   \"url\": \"https://eu-gb.ml.cloud.ibm.com\",\n",
    "                   \"apikey\":\"insert api key here\"\n",
    "                  }\n",
    "\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_metadata = {\n",
    "    'name': 'taketwo-wml-space',\n",
    "    'description': 'taketwo-wml-space',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtained this from WML resource on IBM Cloud\n",
    "space_id = '2d3e8ea7-efa2-4efd-ae26-8a46e7a714df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  -----------------  ------------------------\n",
      "ID                                    NAME               CREATED\n",
      "2d3e8ea7-efa2-4efd-ae26-8a46e7a714df  taketwo-wml-space  2021-07-16T00:54:38.389Z\n",
      "------------------------------------  -----------------  ------------------------\n"
     ]
    }
   ],
   "source": [
    "client.spaces.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_space(space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofware_spec_uid = client.software_specifications.get_id_by_name(\"default_py3.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "            client.repository.ModelMetaNames.NAME: 'Take Two Scikit model',\n",
    "            client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n",
    "            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n",
    "}\n",
    "\n",
    "published_model = client.repository.store_model(\n",
    "    model=trained_model_NB.clf,\n",
    "    meta_props=metadata,\n",
    "    training_data=None,\n",
    "    training_target=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entity\": {\n",
      "    \"software_spec\": {\n",
      "      \"id\": \"e4429883-c883-42b6-87a8-f419d64088cd\",\n",
      "      \"name\": \"default_py3.7\"\n",
      "    },\n",
      "    \"type\": \"scikit-learn_0.23\"\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"created_at\": \"2021-07-16T16:57:10.046Z\",\n",
      "    \"id\": \"d195ce8b-182f-44e1-a424-817bef2896a4\",\n",
      "    \"modified_at\": \"2021-07-16T16:57:15.264Z\",\n",
      "    \"name\": \"Take Two Scikit model\",\n",
      "    \"owner\": \"IBMid-270006CE7D\",\n",
      "    \"space_id\": \"2d3e8ea7-efa2-4efd-ae26-8a46e7a714df\"\n",
      "  },\n",
      "  \"system\": {\n",
      "    \"warnings\": []\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  ---------------------  ------------------------  -----------------\n",
      "ID                                    NAME                   CREATED                   TYPE\n",
      "d195ce8b-182f-44e1-a424-817bef2896a4  Take Two Scikit model  2021-07-16T16:57:10.002Z  scikit-learn_0.23\n",
      "569f90bf-a5f0-4f9b-9d8c-89a881493c1f  Take Two Scikit model  2021-07-16T16:41:17.002Z  scikit-learn_0.23\n",
      "032f80f3-75a0-4a74-95eb-e5994465cc3f  Take Two Scikit model  2021-07-16T02:44:40.002Z  scikit-learn_0.23\n",
      "------------------------------------  ---------------------  ------------------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "models_details = client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "Synchronous deployment creation for uid: 'd195ce8b-182f-44e1-a424-817bef2896a4' started\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "\n",
      "initializing\n",
      "ready\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_uid='af0cf9ba-97ba-470c-982e-665259df01bd'\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"take-two-deployment\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'af0cf9ba-97ba-470c-982e-665259df01bd'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "deployment_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/af0cf9ba-97ba-470c-982e-665259df01bd/predictions\n"
     ]
    }
   ],
   "source": [
    "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  -------------------  -----  ------------------------\n",
      "GUID                                  NAME                 STATE  CREATED\n",
      "af0cf9ba-97ba-470c-982e-665259df01bd  take-two-deployment  ready  2021-07-16T16:57:29.769Z\n",
      "11633376-4d22-4934-a7ec-6e5cb3a23293  take-two-deployment  ready  2021-07-16T02:45:45.117Z\n",
      "------------------------------------  -------------------  -----  ------------------------\n"
     ]
    }
   ],
   "source": [
    "client.deployments.list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': {'asset': {'id': '032f80f3-75a0-4a74-95eb-e5994465cc3f'},\n",
       "  'custom': {},\n",
       "  'deployed_asset_type': 'model',\n",
       "  'hardware_spec': {'id': 'Not_Applicable', 'name': 'S', 'num_nodes': 1},\n",
       "  'name': 'take-two-deployment',\n",
       "  'online': {},\n",
       "  'space_id': '2d3e8ea7-efa2-4efd-ae26-8a46e7a714df',\n",
       "  'status': {'online_url': {'url': 'https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/11633376-4d22-4934-a7ec-6e5cb3a23293/predictions'},\n",
       "   'state': 'ready'}},\n",
       " 'metadata': {'created_at': '2021-07-16T02:45:45.117Z',\n",
       "  'id': '11633376-4d22-4934-a7ec-6e5cb3a23293',\n",
       "  'modified_at': '2021-07-16T02:45:45.117Z',\n",
       "  'name': 'take-two-deployment',\n",
       "  'owner': 'IBMid-270006CE7D',\n",
       "  'space_id': '2d3e8ea7-efa2-4efd-ae26-8a46e7a714df'}}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.deployments.get_details(deployment_uid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to create input format that is needed for WML\n",
    "count_vect = trained_model_NB.count_vect\n",
    "tf_transformer = trained_model_NB.tf_transformer\n",
    "clf = trained_model_NB.clf\n",
    "X_new_counts = count_vect.transform(marker_test.data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "# predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20830x99873 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 768324 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction with WML\n",
    "\n",
    "import requests\n",
    "\n",
    "# NOTE: you must manually set API_KEY below using information retrieved from your IBM Cloud account.\n",
    "API_KEY = wml_credentials['api_key']\n",
    "token_response = requests.post('https://iam.cloud.ibm.com/identity/token', data={\"apikey\": API_KEY, \"grant_type\": 'urn:ibm:params:oauth:grant-type:apikey'})\n",
    "mltoken = token_response.json()[\"access_token\"]\n",
    "\n",
    "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n",
    "\n",
    "# NOTE: manually define and pass the array(s) of values to be scored in the next line\n",
    "payload_scoring = {\"input_data\": [{\"fields\": [array_of_input_fields], \"values\": [array_of_values_to_be_scored, another_array_of_values_to_be_scored]}]}\n",
    "\n",
    "response_scoring = requests.post('https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/11633376-4d22-4934-a7ec-6e5cb3a23293/predictions?version=2021-07-16?version=2021-07-16', json=payload_scoring, headers={'Authorization': 'Bearer ' + mltoken})\n",
    "print(\"Scoring response\")\n",
    "print(response_scoring.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ----  --------  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "META_PROP NAME         TYPE  REQUIRED  SCHEMA\n",
      "NAME                   str   N\n",
      "INPUT_DATA             list  N         [{'name(optional)': 'string', 'id(optional)': 'string', 'fields(optional)': 'array[string]', 'values': 'array[array[string]]'}]\n",
      "INPUT_DATA_REFERENCES  list  N         [{'id(optional)': 'string', 'name(optional)': 'string', 'type(required)': 'string', 'connection(required)': {'href(required)': 'string'}, 'location(required)': {'bucket': 'string', 'path': 'string'}, 'schema(optional)': {'id(required)': 'string', 'fields(required)': [{'name(required)': 'string', 'type(required)': 'string', 'nullable(optional)': 'string'}]}}]\n",
      "OUTPUT_DATA_REFERENCE  dict  N         {'name(optional)': 'string', 'type(required)': 'string', 'connection(required)': {'href(required)': 'string'}, 'location(required)': {'bucket': 'string', 'path': 'string'}, 'schema(optional)': {'id(required)': 'string', 'fields(required)': [{'name(required)': 'string', 'type(required)': 'string', 'nullable(optional)': 'string'}]}}\n",
      "EVALUATIONS_SPEC       list  N         [{'id(optional)': 'string', 'input_target(optional)': 'string', 'metrics_names(optional)': 'array[string]'}]\n",
      "ENVIRONMENT_VARIABLES  dict  N\n",
      "---------------------  ----  --------  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# this command shows the input format required by the deployed model\n",
    "client.deployments.ScoringMetaNames.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# scoring_payload = {\"input_data\": [{\"values\": [score_0, score_1]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
