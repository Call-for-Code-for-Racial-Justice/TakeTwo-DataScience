{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://taketwo-webapi-route-embrace-dev-js.embrace-dev-ocp43-vpc-7ec5d722a0ab3f463fdc90eeb94dbc70-0000.us-east.containers.appdomain.cloud/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johannasaladas/Documents/taketwo/taketwo-datascience'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(API_URL + \"categories\")\n",
    "if response.status_code == 200:\n",
    "    categories = [category[\"name\"] for category in response.json()]\n",
    "    categories += [\"racial slur\"] #add additional not in the API manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appropriation',\n",
       " 'stereotyping',\n",
       " 'deflection',\n",
       " 'gaslighting',\n",
       " 'othering',\n",
       " 'racial slur']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "...     categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'filenames', 'target_names', 'target', 'DESCR']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(twenty_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(API_URL + \"mark\")\n",
    "if response.status_code == 200:\n",
    "    raw_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "data_set = namedtuple('data_set', [\"data\", \"urls\", \"target_names\", \"target\"])([],[],categories,[])\n",
    "\n",
    "for row in raw_data:\n",
    "    if row['category'] in categories:\n",
    "        data_set.data.append(row[\"flagged_string\"])\n",
    "        data_set.urls.append(row[\"url\"])\n",
    "        data_set.target.append(categories.index(row['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_set(data=['zigabo', 'niggah', '#WhiteLivesMatter', 'Golliwogg', 'niggah', 'pre-pandemic', 'black up', 'coronavirus', 'jigg', 'niggress', 'carrying on a “unique mining tradition”', 'golliwog', 'jiggabo', 'Mainland-listed technology', 'ahead', 'colored', 'fuzzy-wuzzy', 'golliwogg', 'niggette', 'slaves to royalty', 'send them all back', 'Sed finibus elit vulputate erat vehicula, eget fermentum lectus feugiat. ', 'refusing to stop blacking up their faces', 'boogie', 'coon', 'pickaninny', \"all that ‘jigaboo' music\", 'The New York', 'curtains', 'bouncing', 'jig', 'nigger', 'nigga', 'nigguh', 'niggar', 'coloured', 'straight outta Compton', 'Why don’t they go back and help fix the totally broken and crime infested places from which they came.', 'prison', 'focus', 'packed plane', 'fun than economy', 'Felis donec et odio pellentesque', 'a elit sit amet', 'palace required revisions', ' up to 15 years', 'nigg', 'Markle grew up in an area that was home to the Bloods gang', 'Stop crying about rrrrrraaaaaacism when none exists, you click bait trash.', 'faces blackened by coal dust', 'Bluegum', 'buck', 'Golliwog', 'jigga', 'porch monkey', 'jiggaboo', 'jiggy', 'niggur', 'master', 'was created to collate', '#alllivesmatte', 'Sed dui eros', 'jiggaboo', 'niggz'], urls=['https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'twitter.com/pearltalk_/status/1294269783605747714', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'example.com', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'www.mirror.co.uk/news/uk-news/clog-dancers-kicked-out-morris-22462224', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'twitter.com/next_china/status/1290149595759013888', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'twitter.com/DailyMailUK/status/936130415705907200', 'twitter.com/Relaxed101/status/1292447074278154240', 'www.lipsum.com/feed/html', 'www.mirror.co.uk/news/uk-news/clog-dancers-kicked-out-morris-22462224', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://www.vox.com/2015/2/23/8091851/news-anchor-jigaboo-music', 'lite.cnn.com/en/article/h_31bb1375cc97f4ca07d00d3f3407e3d5', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'www.pinterest.co.uk/pin/340092209352929735/', 'twitter.com/realdonaldtrump/status/1150381395078000643', 'lite.cnn.com/en/article/h_31bb1375cc97f4ca07d00d3f3407e3d5', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'lite.cnn.com/en/article/h_959a52e405c346f573182402b365bbae', 'loremipsum.io/generator/', 'www.lipsum.com/feed/html', 'twitter.com/i/events/1290451994491023360', 'twitter.com/i/events/1290451994491023360', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'www.dailymail.co.uk/news/article-3896180/Prince-Harry-s-girlfriend-actress-Meghan-Markles.html', 'twitter.com/MarkDice/status/1291955278187909120', 'www.mirror.co.uk/news/uk-news/clog-dancers-kicked-out-morris-22462224', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'example.com', 'blacklivesmatter.com/resources/', 'lite.cnn.com/en/article/h_826602702e4e27acb69602106a22bf0b', 'www.lipsum.com/feed/html', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African', 'https://en.wikipedia.org/wiki/List_of_ethnic_slurs_by_ethnicity#African'], target_names=['appropriation', 'stereotyping', 'deflection', 'gaslighting', 'othering', 'racial slur'], target=[5, 5, 3, 5, 5, 1, 0, 2, 5, 5, 0, 5, 5, 3, 1, 5, 5, 5, 5, 1, 4, 3, 1, 5, 5, 5, 5, 2, 4, 0, 5, 5, 5, 5, 5, 5, 1, 4, 1, 0, 2, 3, 2, 2, 3, 0, 5, 1, 3, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 3, 1, 5, 5])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racial slur\n"
     ]
    }
   ],
   "source": [
    "print(data_set.target_names[data_set.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racial slur\n",
      "racial slur\n",
      "gaslighting\n",
      "racial slur\n",
      "racial slur\n",
      "stereotyping\n",
      "appropriation\n",
      "deflection\n",
      "racial slur\n",
      "racial slur\n"
     ]
    }
   ],
   "source": [
    "for t in data_set.target[:10]:\n",
    "...     print(data_set.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(data_set.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 141)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2257x35788 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 365886 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    ">>> tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    ">>> X_train_tf = tf_transformer.transform(X_train_counts)\n",
    ">>> X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> tfidf_transformer = TfidfTransformer()\n",
    ">>> X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    ">>> X_train_tfidf.shape\n",
    "(2257, 35788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    ">>> X_new_counts = count_vect.transform(docs_new)\n",
    ">>> X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x35788 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(docs_new, predicted):\n",
    "...     print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    ">>> text_clf = Pipeline([\n",
    "...     ('vect', CountVectorizer()),\n",
    "...     ('tfidf', TfidfTransformer()),\n",
    "...     ('clf', MultinomialNB()),\n",
    "... ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    ">>> twenty_test = fetch_20newsgroups(subset='test',\n",
    "...     categories=categories, shuffle=True, random_state=42)\n",
    ">>> docs_test = twenty_test.data\n",
    ">>> predicted = text_clf.predict(docs_test)\n",
    ">>> np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf = Pipeline([\n",
    "...     ('vect', CountVectorizer()),\n",
    "...     ('tfidf', TfidfTransformer()),\n",
    "...     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                           alpha=1e-3, random_state=42,\n",
    "...                           max_iter=5, tol=None)),\n",
    "... ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9101198402130493"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 3, 0, 1, 3, 1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 3, 0, 1, 3, 2, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 3, 0, 1, 3, 1, 2, 1, 3, 0, 3, 1, 2, 1, 3, 0, 3, 2, 3,\n",
       "       1, 0, 2, 1, 1, 3, 2, 0, 1, 3, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 2, 3,\n",
       "       2, 2, 3, 1, 3, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 3, 0, 1, 3, 2, 2, 1, 3, 2, 3, 1, 0, 1, 3, 0, 0, 2, 3,\n",
       "       1, 0, 2, 1, 1, 3, 2, 0, 1, 3, 0, 2, 2, 0, 2, 0, 2, 1, 2, 0, 2, 3,\n",
       "       2, 2, 3, 1, 3, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test.target[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.80      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.94      0.89      0.91       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.91      1502\n",
      "             macro avg       0.91      0.91      0.91      1502\n",
      "          weighted avg       0.91      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    ">>> print(metrics.classification_report(twenty_test.target, predicted,\n",
    "...     target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256,  11,  16,  36],\n",
       "       [  4, 380,   3,   2],\n",
       "       [  5,  35, 353,   3],\n",
       "       [  5,  11,   4, 378]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\n",
    "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...     'tfidf__use_idf': (True, False),\n",
    "...     'clf__alpha': (1e-2, 1e-3),\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naokieabe/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "...     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
